{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 2/10 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 3/10 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 4/10 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 5/10 finished after 200 episode steps with total reward = -3289.000000.\n",
      "Episode 6/10 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 7/10 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 8/10 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 9/10 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 10/10 finished after 200 episode steps with total reward = -3121.000000.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MountainCar-v0 -- Deep Q-learning\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, batch_size=32, memory_size=50000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.training = 10000  # training after 10000 env steps\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        # Updates the online network weights after enough data is collected\n",
    "        if self.training >= len(self.memory):\n",
    "            return\n",
    "\n",
    "        # Samples a batch from the memory\n",
    "        random_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = random_batch[i][0]\n",
    "            action.append(random_batch[i][1])\n",
    "            reward.append(random_batch[i][2])\n",
    "            next_state[i] = random_batch[i][3]\n",
    "            done.append(random_batch[i][4])\n",
    "\n",
    "        # Batch prediction to save compute costs\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model(next_state)\n",
    "\n",
    "        for i in range(len(random_batch)):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        self.model.fit(\n",
    "            np.array(state),\n",
    "            np.array(target),\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def load_weights(self, weights_file):\n",
    "        self.epsilon = self.epsilon_min\n",
    "        self.model.load_weights(weights_file)\n",
    "\n",
    "    def save_weights(self, weights_file):\n",
    "        self.model.save_weights(weights_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Flag used to enable or disable screen recording\n",
    "    recording_is_enabled = True\n",
    "\n",
    "    # Initializes the environment\n",
    "    env = gym.make('MountainCar-v0')\n",
    "\n",
    "    # Records the environment\n",
    "    if recording_is_enabled:\n",
    "        env = gym.wrappers.Monitor(env, \"recording\", video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "    # Defines training related constants\n",
    "    num_episodes = 10\n",
    "    num_episode_steps = env.spec.max_episode_steps  # constant value\n",
    "    action_size = env.action_space.n\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    max_reward = 0\n",
    "\n",
    "    # Creates the agent\n",
    "    agent = Agent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "    # Loads the weights\n",
    "    if os.path.isfile(\"mountain-car-v0.h5\"):\n",
    "        agent.load_weights(\"mountain-car-v0.h5\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Defines the total reward per episode\n",
    "        total_reward = 0\n",
    "\n",
    "        # Resets the environment\n",
    "        observation = env.reset()\n",
    "\n",
    "        # Gets the state\n",
    "        state = np.reshape(observation, [1, state_size])\n",
    "\n",
    "        for episode_step in range(num_episode_steps):\n",
    "            # Renders the screen after new environment observation\n",
    "            env.render(mode=\"human\")\n",
    "\n",
    "            # Gets a new action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Takes action and calculates the total reward\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Recalculates the reward\n",
    "            if observation[1] > state[0][1] >= 0 and observation[1] >= 0:\n",
    "                reward = 20\n",
    "            if observation[1] < state[0][1] <= 0 and observation[1] <= 0:\n",
    "                reward = 20\n",
    "            if done and episode_step < num_episode_steps - 1:\n",
    "                reward += 10000\n",
    "            else:\n",
    "                reward -= 25\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Gets the next state\n",
    "            next_state = np.reshape(observation, [1, state_size])\n",
    "\n",
    "            # Memorizes the experience\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "\n",
    "            # Updates the state\n",
    "            state = next_state\n",
    "\n",
    "            # Updates the network weights\n",
    "            agent.experience_replay()\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode %d/%d finished after %d episode steps with total reward = %f.\"\n",
    "                      % (episode + 1, num_episodes, episode_step + 1, total_reward))\n",
    "                break\n",
    "\n",
    "            elif episode_step >= num_episode_steps - 1:\n",
    "                print(\"Episode %d/%d timed out at %d with total reward = %f.\"\n",
    "                      % (episode + 1, num_episodes, episode_step + 1, total_reward))\n",
    "\n",
    "        # Saves the weights\n",
    "        if total_reward >= max_reward:\n",
    "            agent.save_weights(\"mountain-car-v0.h5\")\n",
    "            max_reward = total_reward\n",
    "\n",
    "    # Closes the environment\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd5508c2ffc7f17f7d31cf4086cc872f89e96996a08987e995649e5fbe85a3a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
