{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym\n",
    "#pip install torch\n",
    "#pip install torchvision\n",
    "#pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeight(x_position):\n",
    "    return np.sin(3 * x_position) * .45 + .55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newreward(pos):\n",
    "    if(pos >= 0.5):\n",
    "        return 2\n",
    "    else:\n",
    "        return (pos+1.2)/1.8 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-learning function\n",
    "def QLearning(env, learning, epsilon, min_eps, episodes):\n",
    "    #Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 50])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "\n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 0,\n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    Qinit = np.copy(Q)\n",
    "\n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "\n",
    "    # Make copy of epsilon\n",
    "    epsl = epsilon\n",
    "\n",
    "    #Keep track of first success\n",
    "    first = episodes + 1\n",
    "\n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "\n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 50])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "\n",
    "        while done != True:\n",
    "            # Render environment for last few episodes\n",
    "            if i >= (episodes - 5) or i<5:\n",
    "                env.render()\n",
    "\n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]])\n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action)\n",
    "\n",
    "            # Discrtize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10,50])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "\n",
    "            # Save to Qpoints\n",
    "            row = np.array([state_adj[0],state_adj[1],action])\n",
    "\n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "\n",
    "            #Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(newreward(state2[0]) + np.max(Q[state2_adj[0], state2_adj[1]]) - Q[state_adj[0]], - [state2_adj[1]])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "\n",
    "            #Notifies of any clears\n",
    "            if state[0]>=0.5 and i<first:\n",
    "                    first = i\n",
    "                    print('First clear on episode {}'.format(i+1))\n",
    "\n",
    "            # Update variables\n",
    "            tot_reward += newreward(state2[0])\n",
    "            state_adj = state2_adj\n",
    "\n",
    "        # Decay epilson\n",
    "        if epsilon > min_eps:\n",
    "            epsilon *= epsl #epsl\n",
    "\n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Episode {} Average Reward: {}' .format(i+1, ave_reward))\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return ave_reward_list, Q, Qinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Q-learning algorithm\n",
    "env.reset()\n",
    "rewards, Qpts, Qinit = QLearning(env, 0.2, 0.999, 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
